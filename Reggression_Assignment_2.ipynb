{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Q1. R-squared in Linear Regression Models\n",
        "# R-squared, also known as the coefficient of determination, is a statistical measure of how well the regression model fits the data.\n",
        "# It is calculated as:\n",
        "# R² = 1 - (SS_res / SS_tot)\n",
        "# Where:\n",
        "# - SS_res is the sum of squared residuals (the difference between the observed and predicted values).\n",
        "# - SS_tot is the total sum of squares (the difference between the observed values and the mean of the observed values).\n",
        "# R-squared represents the proportion of variance in the dependent variable that is explained by the independent variables.\n",
        "# A higher R-squared indicates a better fit of the model to the data.\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Example for calculating R-squared\n",
        "X = [[1], [2], [3], [4], [5]]  # Independent variable\n",
        "y = [1, 2, 3, 4, 5]            # Dependent variable\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "r_squared = r2_score(y, y_pred)\n",
        "print(f\"R-squared: {r_squared}\")\n",
        "\n",
        "# Q2. Adjusted R-squared\n",
        "# Adjusted R-squared is a modified version of R-squared that accounts for the number of independent variables in the model.\n",
        "# It is useful when comparing models with different numbers of predictors. Unlike R-squared, Adjusted R-squared penalizes the addition of irrelevant predictors.\n",
        "# Formula:\n",
        "# Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - k - 1)]\n",
        "# Where:\n",
        "# - n is the number of data points\n",
        "# - k is the number of independent variables\n",
        "\n",
        "# Calculation of Adjusted R-squared\n",
        "n = len(X)\n",
        "k = X[0].__len__()\n",
        "adjusted_r_squared = 1 - ((1 - r_squared) * (n - 1) / (n - k - 1))\n",
        "print(f\"Adjusted R-squared: {adjusted_r_squared}\")\n",
        "\n",
        "# Q3. When to Use Adjusted R-squared\n",
        "# Adjusted R-squared is more appropriate when comparing models with different numbers of predictors or when adding more features to the model.\n",
        "# It is especially useful when you want to avoid overfitting, as it adjusts for the number of features.\n",
        "# Regular R-squared always increases when new features are added, even if they are irrelevant, but Adjusted R-squared may decrease, indicating overfitting.\n",
        "\n",
        "# Q4. RMSE, MSE, and MAE in Regression Analysis\n",
        "# RMSE (Root Mean Squared Error) is the square root of the average squared differences between the predicted and actual values.\n",
        "# MSE (Mean Squared Error) is the average of the squared differences between predicted and actual values.\n",
        "# MAE (Mean Absolute Error) is the average of the absolute differences between predicted and actual values.\n",
        "# These metrics help in evaluating the model's performance in regression analysis.\n",
        "\n",
        "# RMSE = sqrt(mean((y_pred - y_actual)²))\n",
        "# MSE = mean((y_pred - y_actual)²)\n",
        "# MAE = mean(abs(y_pred - y_actual))\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Example\n",
        "rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "mae = mean_absolute_error(y, y_pred)\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"MAE: {mae}\")\n",
        "\n",
        "# Q5. Advantages and Disadvantages of Using RMSE, MSE, and MAE\n",
        "# RMSE:\n",
        "# - Advantages: Gives higher weight to larger errors, so it is sensitive to outliers.\n",
        "# - Disadvantages: Can be disproportionately large when there are significant outliers.\n",
        "# MSE:\n",
        "# - Advantages: Easy to calculate, and like RMSE, it penalizes large errors.\n",
        "# - Disadvantages: Sensitive to outliers, as it squares the errors.\n",
        "# MAE:\n",
        "# - Advantages: Not sensitive to outliers, providing a more general measure of error.\n",
        "# - Disadvantages: Less sensitive to large errors and may not capture important patterns as RMSE or MSE.\n",
        "\n",
        "# Q6. Lasso Regularization\n",
        "# Lasso (Least Absolute Shrinkage and Selection Operator) is a regularization technique that uses L1 regularization.\n",
        "# It adds a penalty equal to the absolute value of the coefficients to the loss function, which can shrink some coefficients to zero, effectively performing feature selection.\n",
        "\n",
        "# Ridge regularization, on the other hand, uses L2 regularization (squared coefficients), which tends to shrink coefficients but does not eliminate them.\n",
        "# Lasso is more appropriate when you expect that only a few features are relevant, as it can set some coefficients to zero, simplifying the model.\n",
        "\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "\n",
        "# Example: Lasso vs Ridge\n",
        "lasso = Lasso(alpha=0.1)\n",
        "ridge = Ridge(alpha=0.1)\n",
        "\n",
        "lasso.fit(X, y)\n",
        "ridge.fit(X, y)\n",
        "\n",
        "print(f\"Lasso Coefficients: {lasso.coef_}\")\n",
        "print(f\"Ridge Coefficients: {ridge.coef_}\")\n",
        "\n",
        "# Q7. How Regularized Linear Models Help Prevent Overfitting\n",
        "# Regularized linear models (Lasso and Ridge) help prevent overfitting by adding a penalty to the loss function, reducing the magnitude of model coefficients.\n",
        "# This encourages simpler models with fewer parameters, which generalizes better to unseen data.\n",
        "# Example: Ridge regularization helps prevent overfitting when there are many correlated features.\n",
        "\n",
        "# Q8. Limitations of Regularized Linear Models\n",
        "# Regularized linear models may not always be the best choice when:\n",
        "# - The relationship between the predictors and the dependent variable is not linear.\n",
        "# - The model is too simple to capture complex patterns, as regularization could remove important features.\n",
        "# - For small datasets, regularization may remove too many features, leading to underfitting.\n",
        "\n",
        "# Q9. Comparing Models with RMSE and MAE\n",
        "# RMSE gives higher weight to larger errors, while MAE treats all errors equally.\n",
        "# If Model A has an RMSE of 10 and Model B has an MAE of 8, it’s important to understand what types of errors are more significant in your context.\n",
        "# RMSE is preferable when large errors are more problematic, while MAE is better when all errors are equally important.\n",
        "\n",
        "# In this case, if you care more about larger errors, Model A might be better, but if you prefer a model that performs consistently across all data points, Model B could be preferred.\n",
        "\n",
        "# Q10. Comparing Models with Ridge and Lasso Regularization\n",
        "# Ridge regularization tends to shrink coefficients evenly, while Lasso can eliminate some features by setting their coefficients to zero.\n",
        "# If you suspect many features are irrelevant, Lasso might be a better choice, as it performs automatic feature selection.\n",
        "# Ridge is better when you believe all features contribute to the model and should not be discarded.\n",
        "# The choice of regularization depends on the nature of your data and whether you believe all features are useful.\n",
        "\n",
        "# Example of model selection using Ridge vs Lasso\n",
        "# Example evaluation using performance metrics or cross-validation to compare the models on a test set.\n"
      ]
    }
  ]
}